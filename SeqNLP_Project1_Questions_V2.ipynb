{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeqNLP_Project1_Questions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xT7MKZuMRaCg"
      },
      "source": [
        "# Sentiment Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wq4RCyyPSYRp"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koxLbo4vbTP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.load.__defaults__=(None, True, True, 'ASCII')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NGCtiXUhSWss",
        "outputId": "d957bf76-0692-4781-d09e-bb5ac075509c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "from keras.datasets import imdb\n",
        "\n",
        "vocab_size = 10000 #vocab size\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size) # vocab_size is no.of words to consider from the dataset, ordering based on frequency.\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCPC_WN-eCyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "vocab_size = 10000 #vocab size\n",
        "maxlen = 20  #number of word used from each review --> making the training faster with 20 words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMEsHYrWxdtk",
        "colab_type": "text"
      },
      "source": [
        "## Train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0g381XzeCyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load dataset as a list of ints\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "#make all sequences of the same length\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test =  pad_sequences(x_test, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy6n-uM2eCy2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d079a0ed-eb7c-4f55-ada6-3d7910c2c379"
      },
      "source": [
        "#Creating the key value pair for word to id and id to word, so that the test/train data (x) can be viewed meaningfully.\n",
        "word_to_id = imdb.get_word_index()\n",
        "word_to_id = {k:v for k,v in word_to_id.items()}"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZhMAgaNeCy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id_to_word = {value:key for key,value in word_to_id.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANhycSehi5-w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "64aa4c6e-c746-4cf1-de78-9517889b2c1e"
      },
      "source": [
        "#Check the sample train data\n",
        "for i in range(0,10):\n",
        "  print (' '.join(id_to_word[id] for id in x_train[i]) , \" ----> \",y_train[i] )"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an  ---->  1\n",
            "are of ship for with of wild to one is very work dark they don't do dvd with those them  ---->  0\n",
            "80's was big also light don't and as it in character looked cinematography so stories is far br man acting  ---->  0\n",
            "br halfway to of took work 20 br similar more he good flower for hit at coming not see reputation  ---->  1\n",
            "for film's was tale have flash but look part i'm film as to penelope is script hard br only acting  ---->  0\n",
            "i i slowly lot of above and with connect in of script their that out end his and i i  ---->  0\n",
            "movies get are and br yes female just its because many br of overly to descent people time very bland  ---->  1\n",
            "once arts like have then own is ebay has have one is you for off his dutch we they an  ---->  0\n",
            "that hilarious not was into through to why for as it by br of where suits was one your life  ---->  1\n",
            "do period it couple in college in viewers get br of my to of material it yet br out more  ---->  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxmCQouku-G-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dybtUgUReCy8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Build Keras Embedding Layer Model\n",
        "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
        "\n",
        "* The embedding layer can be used at the start of a larger deep learning model. \n",
        "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
        "* Use the embedding layer to train our own word2vec models.\n",
        "\n",
        "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5OLM4eBeCy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Model parameters\n",
        "max_features = 20000\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, Embedding\n",
        "from tensorflow.python.keras.layers import LSTM\n",
        "\n",
        "#Building Sequential model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxNDNhrseCzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3CSVVPPeCzD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "72f559b2-1873-42a8-8be2-dff1d76b64e5"
      },
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=15,\n",
        "          validation_data=(x_test, y_test))\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "782/782 [==============================] - 81s 104ms/step - loss: 0.5214 - accuracy: 0.7311 - val_loss: 0.4788 - val_accuracy: 0.7666\n",
            "Epoch 2/15\n",
            "782/782 [==============================] - 81s 104ms/step - loss: 0.4000 - accuracy: 0.8139 - val_loss: 0.4822 - val_accuracy: 0.7644\n",
            "Epoch 3/15\n",
            "782/782 [==============================] - 83s 106ms/step - loss: 0.3226 - accuracy: 0.8558 - val_loss: 0.5039 - val_accuracy: 0.7601\n",
            "Epoch 4/15\n",
            "782/782 [==============================] - 80s 102ms/step - loss: 0.2519 - accuracy: 0.8920 - val_loss: 0.7034 - val_accuracy: 0.7505\n",
            "Epoch 5/15\n",
            "782/782 [==============================] - 79s 102ms/step - loss: 0.1909 - accuracy: 0.9206 - val_loss: 0.7030 - val_accuracy: 0.7524\n",
            "Epoch 6/15\n",
            "782/782 [==============================] - 80s 102ms/step - loss: 0.1398 - accuracy: 0.9460 - val_loss: 0.8200 - val_accuracy: 0.7402\n",
            "Epoch 7/15\n",
            "782/782 [==============================] - 79s 101ms/step - loss: 0.1039 - accuracy: 0.9608 - val_loss: 0.9626 - val_accuracy: 0.7442\n",
            "Epoch 8/15\n",
            "782/782 [==============================] - 78s 100ms/step - loss: 0.0768 - accuracy: 0.9712 - val_loss: 1.1257 - val_accuracy: 0.7269\n",
            "Epoch 9/15\n",
            "782/782 [==============================] - 78s 100ms/step - loss: 0.0559 - accuracy: 0.9796 - val_loss: 1.3619 - val_accuracy: 0.7331\n",
            "Epoch 10/15\n",
            "782/782 [==============================] - 79s 102ms/step - loss: 0.0466 - accuracy: 0.9835 - val_loss: 1.3577 - val_accuracy: 0.7350\n",
            "Epoch 11/15\n",
            "782/782 [==============================] - 80s 103ms/step - loss: 0.0348 - accuracy: 0.9880 - val_loss: 1.4818 - val_accuracy: 0.7390\n",
            "Epoch 12/15\n",
            "782/782 [==============================] - 80s 103ms/step - loss: 0.0276 - accuracy: 0.9901 - val_loss: 1.6031 - val_accuracy: 0.7327\n",
            "Epoch 13/15\n",
            "782/782 [==============================] - 79s 101ms/step - loss: 0.0275 - accuracy: 0.9905 - val_loss: 1.6223 - val_accuracy: 0.7284\n",
            "Epoch 14/15\n",
            "782/782 [==============================] - 80s 102ms/step - loss: 0.0222 - accuracy: 0.9926 - val_loss: 1.7974 - val_accuracy: 0.7250\n",
            "Epoch 15/15\n",
            "782/782 [==============================] - 79s 101ms/step - loss: 0.0173 - accuracy: 0.9942 - val_loss: 1.6946 - val_accuracy: 0.7307\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdadbb0ed30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXPQU9Smk3Pw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b1ffbb1e-7722-4fc8-e652-c8b164fe8ca1"
      },
      "source": [
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 7s 9ms/step - loss: 1.6946 - accuracy: 0.7307\n",
            "Test score: 1.6945756673812866\n",
            "Test accuracy: 0.7307199835777283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07IoLr8hygv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, recall_score, precision_score,accuracy_score, f1_score,roc_auc_score\n",
        "import pandas as pd\n",
        "\n",
        "def binary_classification_performance(y_test, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "    accuracy = round(accuracy_score(y_pred = y_pred, y_true = y_test),2)\n",
        "    precision = round(precision_score(y_pred = y_pred, y_true = y_test),2)\n",
        "    recall = round(recall_score(y_pred = y_pred, y_true = y_test),2)\n",
        "    f1_score = round(2*precision*recall/(precision + recall),2)\n",
        "    specificity = round(tn/(tn+fp),2)\n",
        "    npv = round(tn/(tn+fn),2)\n",
        "    auc_roc = round(roc_auc_score(y_score = y_pred, y_true = y_test),2)\n",
        "\n",
        "\n",
        "    result = pd.DataFrame({'Accuracy' : [accuracy],\n",
        "                         'Precision or PPV' : [precision],\n",
        "                         'Recall or senitivity or TPR' : [recall],\n",
        "                         'f1 score' : [f1_score],\n",
        "                         'AUC_ROC' : [auc_roc],\n",
        "                         'Specificty or TNR': [specificity],\n",
        "                         'NPV' : [npv],\n",
        "                         'True Positive' : [tp],\n",
        "                         'True Negative' : [tn],\n",
        "                         'False Positive':[fp],\n",
        "                         'False Negative':[fn]})\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC3NgW3yx3M7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trA5VT_uy15j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51fdb592-48ce-442c-f863-cf432801dc19"
      },
      "source": [
        "y_pred_upd = [0 if val <0.5 else 1 for val in y_pred]"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4M4ILczyowY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "5a010082-f580-41d1-fe4f-f73208866594"
      },
      "source": [
        "binary_classification_performance(y_test, y_pred_upd)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision or PPV</th>\n",
              "      <th>Recall or senitivity or TPR</th>\n",
              "      <th>f1 score</th>\n",
              "      <th>AUC_ROC</th>\n",
              "      <th>Specificty or TNR</th>\n",
              "      <th>NPV</th>\n",
              "      <th>True Positive</th>\n",
              "      <th>True Negative</th>\n",
              "      <th>False Positive</th>\n",
              "      <th>False Negative</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.73</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.75</td>\n",
              "      <td>9582</td>\n",
              "      <td>8686</td>\n",
              "      <td>3814</td>\n",
              "      <td>2918</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Accuracy  Precision or PPV  ...  False Positive  False Negative\n",
              "0      0.73              0.72  ...            3814            2918\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igq8Qm8GeCzG",
        "colab_type": "text"
      },
      "source": [
        "## Retrive the output of each layer in keras for a given single test sample from the trained model you built"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AqOnLa2eCzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "inp = model.input                                           # input placeholder\n",
        "outputs = [layer.output for layer in model.layers]          # all layer outputs\n",
        "functors = [K.function([inp], [out]) for out in outputs]    # evaluation functions\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG98TsnIrdNO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6561ea68-9528-4284-9016-21c7433c5ad4"
      },
      "source": [
        "# Testing\n",
        "layer_outs = [func(x_test) for func in functors]\n",
        "print (layer_outs[0])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[[ 1.99849814e-01, -2.87888125e-02, -1.28157750e-01, ...,\n",
            "          1.13628805e-01,  1.60491168e-01,  1.31762773e-01],\n",
            "        [-5.93710579e-02, -1.11400321e-01, -1.29733920e-01, ...,\n",
            "         -1.08561909e-03,  3.37580554e-02,  1.01813294e-01],\n",
            "        [ 1.58350449e-02, -7.62492791e-02, -2.71570459e-02, ...,\n",
            "          3.66376303e-02,  7.42851570e-02, -3.77052575e-02],\n",
            "        ...,\n",
            "        [ 3.96433026e-02,  1.09101467e-01, -4.15450670e-02, ...,\n",
            "          1.37761965e-01, -3.25513519e-02,  1.14513509e-01],\n",
            "        [ 3.09907608e-02, -7.10750185e-03, -4.18368615e-02, ...,\n",
            "          2.18732748e-02, -4.92075905e-02, -1.92552485e-04],\n",
            "        [-1.20647117e-01, -1.39270589e-01, -5.02942428e-02, ...,\n",
            "         -1.02081321e-01,  6.09316118e-03, -3.94395553e-04]],\n",
            "\n",
            "       [[ 5.04734330e-02, -9.61201712e-02, -1.63638815e-02, ...,\n",
            "         -1.20148470e-03, -3.25324461e-02, -9.80055612e-03],\n",
            "        [ 5.04734330e-02, -9.61201712e-02, -1.63638815e-02, ...,\n",
            "         -1.20148470e-03, -3.25324461e-02, -9.80055612e-03],\n",
            "        [-7.36139342e-02, -1.47139624e-01, -1.29833326e-01, ...,\n",
            "         -1.98733173e-02,  2.44119186e-02, -5.40080778e-02],\n",
            "        ...,\n",
            "        [ 1.35777071e-01, -1.49964169e-01, -4.58573699e-02, ...,\n",
            "          5.85093908e-02, -7.90209547e-02,  5.85878789e-02],\n",
            "        [ 1.54012050e-02,  1.72372740e-02, -4.25373279e-02, ...,\n",
            "          7.67287472e-03,  3.19564827e-02, -8.25578440e-03],\n",
            "        [-8.01076218e-02,  1.09135054e-01,  2.93928891e-01, ...,\n",
            "         -3.92317325e-01, -1.87477216e-01, -2.41475984e-01]],\n",
            "\n",
            "       [[ 7.56677911e-02, -7.49172866e-02, -1.11538425e-01, ...,\n",
            "         -2.69167754e-03,  4.21384722e-02,  4.38026711e-03],\n",
            "        [ 8.28286111e-02, -8.07813182e-02, -1.83669142e-02, ...,\n",
            "          1.15706343e-02,  3.03804446e-02,  2.62026228e-02],\n",
            "        [-4.61193137e-02,  5.89322709e-02,  7.52675310e-02, ...,\n",
            "         -5.90140559e-02,  7.97536224e-02,  4.81587090e-03],\n",
            "        ...,\n",
            "        [ 1.04247063e-01,  1.55048566e-02,  5.29964305e-02, ...,\n",
            "          1.04581341e-01, -5.21013364e-02,  3.09820753e-03],\n",
            "        [ 1.37609780e-01, -1.76432982e-01, -4.50914688e-02, ...,\n",
            "         -1.48907572e-01,  7.74820149e-02,  9.15933698e-02],\n",
            "        [ 1.17116965e-01, -9.70743746e-02, -3.43937650e-02, ...,\n",
            "          2.08474398e-01,  6.23206496e-02,  1.86724678e-01]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[ 2.11286172e-02,  1.42296252e-03,  1.17847594e-02, ...,\n",
            "         -4.29435354e-03, -1.31438337e-02, -2.85164081e-02],\n",
            "        [ 6.13006838e-02, -2.26381849e-02,  1.24743553e-02, ...,\n",
            "          9.81272906e-02, -2.86719725e-02,  1.48787843e-02],\n",
            "        [-9.32429079e-03,  2.67879572e-02,  5.69534712e-02, ...,\n",
            "         -1.35405734e-01,  7.00091152e-03, -3.86800356e-02],\n",
            "        ...,\n",
            "        [-4.25749877e-03,  4.71338145e-02, -9.54766795e-02, ...,\n",
            "          3.57274758e-03, -1.45833706e-02, -3.30267218e-03],\n",
            "        [-4.66729924e-02, -8.39549825e-02,  4.15489413e-02, ...,\n",
            "         -2.00777175e-03, -1.09456129e-01,  8.60347673e-02],\n",
            "        [ 1.22987561e-01, -1.17253169e-01, -1.60857126e-01, ...,\n",
            "          1.36736959e-01,  5.76968864e-03,  8.48558396e-02]],\n",
            "\n",
            "       [[-9.02679339e-02,  2.90307775e-02,  1.30110700e-02, ...,\n",
            "         -2.74981540e-02, -9.29809548e-03,  5.08728623e-02],\n",
            "        [ 6.10245466e-02,  4.51198826e-03,  2.04629637e-02, ...,\n",
            "         -1.08675905e-01,  1.90963894e-02, -1.49112523e-01],\n",
            "        [ 1.93625828e-03,  2.74316985e-02,  2.86466368e-02, ...,\n",
            "          7.39296898e-02,  1.63624436e-02, -4.76309545e-02],\n",
            "        ...,\n",
            "        [-2.27409638e-02, -3.23468782e-02, -1.02106012e-01, ...,\n",
            "         -1.66386701e-02, -1.26968995e-01, -1.19272485e-01],\n",
            "        [ 5.60486689e-02, -3.29994485e-02, -8.01624209e-02, ...,\n",
            "          2.70413850e-02,  3.58087360e-03,  3.75569724e-02],\n",
            "        [ 1.64910302e-01, -2.55997330e-01, -7.09399134e-02, ...,\n",
            "          5.68034919e-03,  2.66677856e-01,  1.02495171e-01]],\n",
            "\n",
            "       [[-5.36508001e-02, -2.96800025e-02,  9.68339108e-03, ...,\n",
            "         -8.43395963e-02,  1.32957473e-01,  5.11037782e-02],\n",
            "        [ 3.96433026e-02,  1.09101467e-01, -4.15450670e-02, ...,\n",
            "          1.37761965e-01, -3.25513519e-02,  1.14513509e-01],\n",
            "        [ 6.13006838e-02, -2.26381849e-02,  1.24743553e-02, ...,\n",
            "          9.81272906e-02, -2.86719725e-02,  1.48787843e-02],\n",
            "        ...,\n",
            "        [ 7.56677911e-02, -7.49172866e-02, -1.11538425e-01, ...,\n",
            "         -2.69167754e-03,  4.21384722e-02,  4.38026711e-03],\n",
            "        [-1.05673507e-01,  8.22061971e-02,  1.01562319e-02, ...,\n",
            "         -2.05717295e-01,  2.26276536e-02, -1.36103362e-01],\n",
            "        [ 7.00068101e-02,  1.16606914e-01,  4.23523039e-02, ...,\n",
            "         -5.33865159e-03, -1.26740048e-02, -4.92027774e-03]]],\n",
            "      dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}